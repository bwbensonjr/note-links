# CLAUDE.md - Design and Implementation Notes

This document describes the design decisions and implementation details of the Obsidian Link Extractor.

## Project Structure

```
note-links/
├── .env                        # Local config: DAILY_NOTES_PATH (not committed)
├── .env.example                # Template for .env
├── config.yaml                 # Runtime configuration (rate limits, Bedrock settings)
├── pyproject.toml              # Dependencies and CLI entry point
├── links.db                    # SQLite3 database (created on first run)
├── docs/
│   ├── index.html              # Static site (Vue.js SPA for GitHub Pages)
│   ├── data.json               # Exported link data for static site
│   └── feed.xml                # RSS 2.0 feed for static site
├── src/link_extractor/
│   ├── main.py                 # CLI and pipeline orchestration
│   ├── config.py               # Configuration loading (.env + YAML)
│   ├── extraction/
│   │   ├── parser.py           # Markdown link parsing
│   │   └── scanner.py          # Daily notes directory traversal
│   ├── fetching/
│   │   ├── fetcher.py          # Async HTTP with rate limiting
│   │   ├── content.py          # HTML text extraction
│   │   └── pdf.py              # PDF text extraction
│   ├── summarization/
│   │   ├── base.py             # Abstract summarizer interface
│   │   └── bedrock.py          # AWS Bedrock Claude implementation
│   ├── tagging/
│   │   └── llm_tagger.py       # LLM-based auto-tagging via Bedrock
│   ├── export/
│   │   └── rss.py              # RSS 2.0 feed generation
│   └── storage/
│       ├── models.py           # Dataclasses (ExtractedLink, LinkRecord, Tag)
│       └── database.py         # SQLite operations with FTS5
└── tests/
```

## Design Decisions

### Configuration: .env + YAML

Configuration uses a layered approach:
- **`.env` file**: For machine-specific paths (`DAILY_NOTES_PATH`, `DATABASE_PATH`). Loaded via `python-dotenv` at module initialization.
- **`config.yaml`**: For runtime settings (rate limits, Bedrock model, batch size).

Environment variables take precedence over YAML values. This allows the repo to be cloned and used immediately by setting only the `.env` file, without modifying tracked files.

### Storage: SQLite with FTS5

Chose SQLite over JSON files for:
- **Queryability**: SQL enables complex filtering by date, tag, domain
- **Full-text search**: FTS5 extension provides fast text search with ranking
- **Incremental updates**: Easy to track processed files via hash
- **Single file**: Portable, no server required

### Summarization: AWS Bedrock

Uses Bedrock rather than direct Anthropic API because the user's existing setup uses Bedrock for Claude Code. Key implementation details:
- Uses cross-region inference profiles (`us.anthropic.claude-3-5-sonnet-20241022-v2:0`)
- Boto3's sync API wrapped in `run_in_executor` for async compatibility
- Content truncated to 8000 chars to stay within context limits

### Link Parsing Strategy

The parser handles multiple formats found in the user's notes:
1. Bare URLs: `- description - https://example.com`
2. Markdown links: `- [Title](https://example.com) extra text`
3. Nested/indented links (tracks parent relationships)

Regex-based parsing extracts the `## Links` section, then processes each list item.

### Rate Limiting

Per-domain rate limiting (default 1 req/sec) to be respectful of web servers:
- Tracks last request time per domain
- Async lock ensures sequential access to timing data
- Configurable via `rate_limit_per_second`

### PDF Handling

PDFs are common in the user's links (academic papers, etc.):
- Downloads to temp file
- Extracts text using PyMuPDF (fast, pure Python bindings to MuPDF)
- Truncates to 50k chars for summarization

### Auto-Tagging

LLM-based tagging using Bedrock Claude:
- Tags are generated by the LLM based on page content and metadata
- Only processes links that don't already have tags (incremental)
- Tags stored with confidence scores and source (`llm`)

### Content Extraction

HTML content extraction tries multiple strategies in order:
1. `<article>` tag
2. `<main>` tag
3. `<div>` with class matching `content|article|post`
4. `<body>` as fallback

If a matched element is empty, it falls back to the next candidate. This handles sites where wrapper divs match the pattern but contain no text.

### Summarization Fallbacks

When page content cannot be extracted (JS-heavy sites, login walls, etc.):
- Links are still marked as `fetch_status = 'success'` if HTTP 200
- Summarization creates a metadata-based summary from `page_title` and `description`
- These are marked with `summarizer_model = 'metadata'` to distinguish from LLM summaries

## Database Schema

```sql
CREATE TABLE links (
    id INTEGER PRIMARY KEY,
    url TEXT UNIQUE NOT NULL,
    title TEXT,
    description TEXT,           -- User's note from markdown
    domain TEXT,
    source_date DATE,           -- Date of daily note
    source_file TEXT,
    page_title TEXT,            -- From <title> tag
    page_content TEXT,          -- Extracted text
    fetch_status TEXT,          -- not_fetched, success, failed, timeout, skipped
    summary TEXT,
    summarizer_model TEXT,
    ...
);

CREATE VIRTUAL TABLE links_fts USING fts5(
    title, description, page_content, summary
);
```

Triggers keep FTS index synchronized with main table.

## Pipeline Flow

1. **Scan**: Find daily note files matching `YYYY-MM-DD.md` pattern
2. **Parse**: Extract links from `## Links` section
3. **Dedupe**: Skip URLs already in database
4. **Fetch**: Download HTML/PDF content with rate limiting (only unfetched links)
5. **Extract**: Convert HTML to plain text, extract PDF text
6. **Summarize**: Generate summaries via Bedrock Claude (only unsummarized links)
   - Falls back to metadata summary if page content is empty
7. **Tag**: Apply LLM-based auto-tagging (only untagged links)

Each step is independently skippable via CLI flags. The pipeline is fully incremental - running `extract` multiple times will only process new/pending items.

## CLI Commands

```bash
# Main extraction pipeline
uv run link-extractor extract [OPTIONS]
  --no-fetch       Skip fetching web pages
  --no-summarize   Skip summarization
  --no-tag         Skip auto-tagging
  --date-from      Start date (YYYY-MM-DD)
  --date-to        End date (YYYY-MM-DD)

# Re-fetch links that have empty content (after fixing extraction bugs, etc.)
uv run link-extractor refetch [OPTIONS]
  --dry-run        Show what would be refetched without doing it
  --limit N        Limit number of links to refetch

# Re-tag all links (clears and regenerates tags)
uv run link-extractor retag [OPTIONS]
  --clear-existing  Clear existing tags first
  --limit N         Limit number of links

# Search and browse
uv run link-extractor search "query"     # Full-text search
uv run link-extractor by-tag TAG_NAME    # List links by tag
uv run link-extractor tags               # List all tags with counts
uv run link-extractor stats              # Show database statistics

# Export for static site
uv run link-extractor export-json        # Export to docs/data.json
uv run link-extractor export-rss         # Export to docs/feed.xml
  --title TEXT       Override feed title from config
  --description TEXT Override feed description from config
  --limit N          Max items to include
```

## Extending

### Adding a new summarizer

Implement `BaseSummarizer` interface:

```python
class MySummarizer(BaseSummarizer):
    async def summarize(self, content, title, description, url) -> str:
        ...

    @property
    def model_name(self) -> str:
        return "my-model"
```

### Querying the database directly

```bash
sqlite3 links.db "SELECT url, summary FROM links WHERE summary IS NOT NULL LIMIT 5"
```

Full-text search:
```bash
sqlite3 links.db "SELECT url FROM links_fts WHERE links_fts MATCH 'rust AND compiler'"
```
